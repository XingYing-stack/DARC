"""
Difficulty-aware solver reward function with selectable label source.

Two modes for gold label selection:
- rule: use dataset-provided gold from `reward_model = {"style": "rule", "ground_truth": str(answer)}`.
- self_vote: reuse the rollout n samples per prompt (already generated by trainer)
  to do majority voting as the pseudo-label, then score each sample against it.

Interface follows examples/reward_function/math.py:compute_score
and returns a list of dicts with keys {"overall", "format", "accuracy"}.

Usage in CLI:
  worker.reward.reward_function=./examples/reward_function/difficulty_aware_solver.py:compute_score
  (optionally) worker.reward.reward_function_kwargs.solver_label_mode=auto|rule|self_vote
  (optionally) worker.reward.reward_function_kwargs.format_weight=0.05

Environment overrides (optional):
- SOLVER_LABEL_MODE: same as solver_label_mode above, takes precedence if present.
- SOLVER_FORMAT_WEIGHT: float, overrides format_weight.

Notes:
- We do not call the actor again; for self_vote we group the rollout outputs by
  original sample using repeat_interleave semantics (n contiguous items per sample).
- This avoids modifying the trainer and ensures we use the current solver's outputs.
"""

from __future__ import annotations

import json
import os
import re
import ast
from collections import Counter
from typing import Any, Dict, List, Tuple

from mathruler.grader import extract_boxed_content, grade_answer


_BOXED_RE = re.compile(r"\\boxed\{(.*?)\}")


def _env_or_default(name: str, default: str) -> str:
    v = os.getenv(name)
    return v if v is not None and str(v).strip() != "" else default


def _to_canonical_json(obj: Any) -> str:
    """Stable JSON string for grouping equal ground_truth entries."""
    try:
        return json.dumps(obj, sort_keys=True, ensure_ascii=False)
    except Exception:
        return str(obj)


def _parse_reward_model(item: Any) -> Dict[str, Any]:
    """Parse a reward_model cell that may be a dict or a JSON string.

    Expected for rule mode: {"style": "rule", "ground_truth": "..."}
    For self_vote: {"style": "self_vote"} (additional fields are ignored here).
    """
    if isinstance(item, dict):
        # When `INJECT_EXTRA_INFO_TO_GROUND_TRUTH=1`, dataset wraps original answer_key
        # value as: {"ground_truth": <original>, "extra_info": ...}. Unwrap it here so
        # rule-mode can still read {"style": "...", "ground_truth": "..."}.
        if "style" not in item and "ground_truth" in item:
            extra_info = item.get("extra_info")
            parsed = _parse_reward_model(item.get("ground_truth"))
            if extra_info is not None and "extra_info" not in parsed:
                parsed = parsed.copy()
                parsed["extra_info"] = extra_info
            return parsed
        # Some datasets may store the structure under a different key.
        if "reward_model" in item and ("style" not in item or "ground_truth" not in item):
            extra_info = item.get("extra_info")
            parsed = _parse_reward_model(item.get("reward_model"))
            if extra_info is not None and "extra_info" not in parsed:
                parsed = parsed.copy()
                parsed["extra_info"] = extra_info
            return parsed
        return item
    if isinstance(item, str):
        s = item.strip()
        if s.startswith("{") and s.endswith("}"):
            try:
                return json.loads(s)
            except Exception:
                # common in parquet/csv: python dict repr with single quotes
                try:
                    obj = ast.literal_eval(s)
                    if isinstance(obj, dict):
                        return obj
                except Exception:
                    pass
                return {"style": "rule", "ground_truth": s}
        # treat plain string as direct ground truth
        return {"style": "rule", "ground_truth": s}
    # fallback
    return {"style": "rule", "ground_truth": str(item)}


def _extract_answer(text: str) -> str:
    """Extract final answer. Prefer mathruler's boxed content; fallback to last \boxed{} via regex.
    Returns normalized string (stripped)."""
    try:
        ans = extract_boxed_content(text)
        if isinstance(ans, str) and ans.strip():
            return ans.strip()
    except Exception:
        pass

    m = None
    try:
        m = list(_BOXED_RE.finditer(text))
    except Exception:
        m = None
    if m:
        return m[-1].group(1).strip()
    return ""


def _format_score(predict: str, require_think: bool = False) -> float:
    """Return 1.0 if output format contains required markers.

    By default we only require a \boxed{...} to avoid over-penalizing formats.
    If require_think=True, also require <think>...</think> before the boxed answer.
    """
    if require_think:
        # strict pattern: <think>...</think> ... \boxed{...}
        pattern = re.compile(r"<think>.*?</think>.*?\\boxed\{.*?\}", re.DOTALL | re.IGNORECASE)
        return 1.0 if re.search(pattern, predict) else 0.0
    # relaxed: only require boxed
    return 1.0 if _BOXED_RE.search(predict) else 0.0


def _majority_vote(answers: List[str]) -> Tuple[str, float]:
    """Return (majority_answer, fraction). If tie, pick the first seen among the winners."""
    if not answers:
        return "", 0.0
    cnt = Counter(a for a in answers if a != "")
    if not cnt:
        return "", 0.0
    most_common = cnt.most_common()
    top_count = most_common[0][1]
    # tie-break by first occurrence order among answers list
    winners = {a for a, c in most_common if c == top_count}
    for a in answers:
        if a in winners:
            majority = a
            break
    frac = top_count / max(len(answers), 1)
    return majority, frac


def _group_indices_by_repeat(ground_truths: List[Any]) -> List[Tuple[int, int]]:
    """Compute contiguous groups [start, end) where equal ground_truth repeats.

    Reward pipeline uses DataProto.repeat(..., interleave=True), which produces
    n contiguous repeats of each original sample. We exploit that to avoid
    needing uids inside the reward function.
    """
    groups: List[Tuple[int, int]] = []
    if not ground_truths:
        return groups
    # canonicalize for equality check
    keys = [_to_canonical_json(x) for x in ground_truths]
    start = 0
    for i in range(1, len(keys) + 1):
        if i == len(keys) or keys[i] != keys[start]:
            groups.append((start, i))
            start = i
    return groups


def _score_against_gold(pred: str, gold: str, format_weight: float, require_think: bool) -> Dict[str, float]:
    fmt = _format_score(pred, require_think=require_think)
    pred_ans = _extract_answer(pred)
    acc = 0.0
    if pred_ans == "" or pred_ans.lower() == 'none':
        acc = 0.0
    else:
        try:
            acc = 1.0 if grade_answer(pred_ans, gold) else 0.0
        except Exception:
            # fallback to strict string match
            acc = 1.0 if pred_ans != "" and pred_ans == str(gold).strip() else 0.0
    overall = (1.0 - format_weight) * acc + format_weight * fmt
    return {"overall": float(overall), "format": float(fmt), "accuracy": float(acc)}


def compute_score(
    predicts: List[str],
    ground_truths: List[Any],
    *,
    solver_label_mode: str = "auto",  # one of: auto, rule, self_vote
    format_weight: float = 0.05,
    require_think: bool = False,
    # Accept extra kwargs that may be set for trainer-side labeling to avoid TypeError
    label_prompt_key: str | None = None,
    label_n: int | None = None,
    label_temperature: float | None = None,
    label_top_p: float | None = None,
    label_top_k: int | None = None,
    **_: Any,
) -> List[Dict[str, float]]:
    """Compute per-sample scores.

    - When mode is rule, expect ground_truths to carry a dict or JSON string with
      {"style": "rule", "ground_truth": "..."}, or a raw string; score vs that gold.
    - When mode is self_vote, group rollout outputs per original sample and use
      majority vote as pseudo gold; score each sample vs the group majority.
    - auto: if parsed reward_model has style=rule use rule, else fall back to self_vote.
    """

    # env overrides
    solver_label_mode = _env_or_default("SOLVER_LABEL_MODE", solver_label_mode).strip().lower()
    try:
        env_fw = _env_or_default("SOLVER_FORMAT_WEIGHT", str(format_weight))
        format_weight = float(env_fw)
    except Exception:
        pass

    # parse reward_model info (only used for mode decisions / rule gold)
    parsed = [_parse_reward_model(gt) for gt in ground_truths]

    # pre-extract answers for speed
    pred_answers = [_extract_answer(p) for p in predicts]

    results: List[Dict[str, float]] = []

    if solver_label_mode not in ("auto", "rule", "self_vote"):
        solver_label_mode = "auto"

    if solver_label_mode == "rule":
        # simple path: score vs provided ground truth (one per item)
        for pred, info in zip(predicts, parsed):
            gold = str(info.get("ground_truth", ""))
            results.append(_score_against_gold(pred, gold, format_weight, require_think))
        return results

    # self_vote path operates on groups
    groups = _group_indices_by_repeat(parsed)

    # Iterate groups and score within each group
    for (lo, hi) in groups:
        # decide mode if auto: rule when explicit; else self_vote
        group_mode = solver_label_mode
        if group_mode == "auto":
            group_mode = "rule" if str(parsed[lo].get("style", "")).lower() == "rule" else "self_vote"
        # If caller requested self_vote but a (non-empty) gold is present (e.g.,
        # trainer performed text_prompt labeling), prefer rule to avoid double voting.
        if group_mode == "self_vote":
            gold_present = str(parsed[lo].get("ground_truth", "")).strip() != ""
            if gold_present:
                group_mode = "rule"

        if group_mode == "rule":
            gold = str(parsed[lo].get("ground_truth", ""))
            for i in range(lo, hi):
                results.append(_score_against_gold(predicts[i], gold, format_weight, require_think))
            continue

        # self_vote
        group_answers = [pred_answers[i] for i in range(lo, hi)]
        vote_answer, _ = _majority_vote(group_answers)
        for i in range(lo, hi):
            results.append(_score_against_gold(predicts[i], vote_answer, format_weight, require_think))

    return results
